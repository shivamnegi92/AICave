{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033a902b",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Build and Train a LLaMA Model from Scratch\n",
    "\n",
    "You've successfully built and trained a LLaMA model from scratch!\n",
    "\n",
    "## What You Accomplished:\n",
    "\n",
    "âœ… **Built a complete transformer model** with modern architecture\n",
    "- RoPE for position encoding\n",
    "- RMSNorm for stabilization  \n",
    "- Grouped Query Attention for efficiency\n",
    "- SwiGLU activation function\n",
    "\n",
    "âœ… **Trained the model** on text data\n",
    "- Character-level tokenization\n",
    "- AdamW optimizer with cosine scheduling\n",
    "- Proper gradient clipping\n",
    "\n",
    "âœ… **Generated text** with different sampling strategies\n",
    "- Temperature control\n",
    "- Top-k and top-p sampling\n",
    "\n",
    "âœ… **Evaluated and saved** your model\n",
    "- Perplexity metrics\n",
    "- Checkpoint system\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### To Improve Your Model:\n",
    "1. **More Training Data:** Use larger datasets (books, Wikipedia, etc.)\n",
    "2. **Longer Training:** Train for more epochs\n",
    "3. **Bigger Model:** Increase `d_model`, `n_layers`, etc.\n",
    "4. **Better Tokenization:** Use BPE or SentencePiece instead of characters\n",
    "5. **Fine-tuning:** Train on specific tasks or domains\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "- **Multi-GPU Training:** Distribute training across GPUs\n",
    "- **Mixed Precision:** Use FP16 for faster training\n",
    "- **LoRA:** Efficient fine-tuning technique\n",
    "- **RLHF:** Reinforcement Learning from Human Feedback\n",
    "- **Prompt Engineering:** Optimize prompts for better outputs\n",
    "\n",
    "### Real LLaMA Models:\n",
    "This demo used small sizes for learning. Real LLaMA models:\n",
    "- LLaMA 7B: 7 billion parameters\n",
    "- LLaMA 13B: 13 billion parameters  \n",
    "- LLaMA 70B: 70 billion parameters\n",
    "\n",
    "Your model: ~{n_params:,} parameters (much smaller for fast training!)\n",
    "\n",
    "## Resources:\n",
    "- [LLaMA Paper](https://arxiv.org/abs/2302.13971)\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You now understand how modern large language models work! ðŸš€**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "def save_checkpoint(model, tokenizer, config, filepath):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'char_to_idx': tokenizer.char_to_idx,\n",
    "        'idx_to_char': tokenizer.idx_to_char,\n",
    "        'chars': tokenizer.chars\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"âœ“ Model saved to: {filepath}\")\n",
    "\n",
    "# Load checkpoint\n",
    "def load_checkpoint(filepath, device='cpu'):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    # Recreate tokenizer\n",
    "    tokenizer = CharTokenizer(\"\")\n",
    "    tokenizer.chars = checkpoint['chars']\n",
    "    tokenizer.vocab_size = checkpoint['vocab_size']\n",
    "    tokenizer.char_to_idx = checkpoint['char_to_idx']\n",
    "    tokenizer.idx_to_char = checkpoint['idx_to_char']\n",
    "    \n",
    "    # Recreate model\n",
    "    config = checkpoint['config']\n",
    "    model = LLaMA(config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"âœ“ Model loaded from: {filepath}\")\n",
    "    return model, tokenizer, config\n",
    "\n",
    "# Save the trained model\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "save_checkpoint(model, tokenizer, config, 'llama_checkpoint.pt')\n",
    "print(\"\\nâœ“ You can now load this model anytime!\")\n",
    "\n",
    "# Example: Load the model\n",
    "print(\"\\nEXAMPLE: Loading saved model\")\n",
    "print(\"-\" * 60)\n",
    "loaded_model, loaded_tokenizer, loaded_config = load_checkpoint('llama_checkpoint.pt', device)\n",
    "print(\"âœ“ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d43d4e",
   "metadata": {},
   "source": [
    "## Step 15: Save and Load Model\n",
    "\n",
    "Save your trained model so you can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    \"\"\"Calculate perplexity on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += y.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Evaluate on training data\n",
    "print(\"EVALUATING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "perplexity, avg_loss = calculate_perplexity(model, train_loader, device)\n",
    "\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity:   {perplexity:.2f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if perplexity < 10:\n",
    "    print(\"  ðŸŒŸ Excellent! Model learned the patterns well.\")\n",
    "elif perplexity < 50:\n",
    "    print(\"  âœ… Good! Model has reasonable understanding.\")\n",
    "elif perplexity < 100:\n",
    "    print(\"  âš ï¸  Fair. More training could help.\")\n",
    "else:\n",
    "    print(\"  âŒ Poor. Model needs more training or better data.\")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f0e9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Evaluation and Saving\n",
    "\n",
    "## Step 14: Evaluate Model Performance\n",
    "\n",
    "Let's measure how well our model learned!\n",
    "\n",
    "**Perplexity:** A common metric for language models\n",
    "- Lower perplexity = better model\n",
    "- Measures how \"surprised\" the model is by the text\n",
    "- Perfect model would have perplexity = 1\n",
    "\n",
    "**Formula:** Perplexity = exp(average loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc55281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation with different settings\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "prompt = \"Machine learning\"\n",
    "\n",
    "# Example 1: Low temperature (conservative)\n",
    "print(\"1. LOW TEMPERATURE (0.5) - More predictable:\")\n",
    "print(\"-\" * 60)\n",
    "output1 = generate(model, tokenizer, prompt, max_length=150, \n",
    "                   temperature=0.5, device=device)\n",
    "print(output1)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 2: Medium temperature\n",
    "print(\"2. MEDIUM TEMPERATURE (1.0) - Balanced:\")\n",
    "print(\"-\" * 60)\n",
    "output2 = generate(model, tokenizer, prompt, max_length=150,\n",
    "                   temperature=1.0, device=device)\n",
    "print(output2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 3: High temperature (creative)\n",
    "print(\"3. HIGH TEMPERATURE (1.5) - More creative:\")\n",
    "print(\"-\" * 60)\n",
    "output3 = generate(model, tokenizer, prompt, max_length=150,\n",
    "                   temperature=1.5, device=device)\n",
    "print(output3)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 4: With top-k sampling\n",
    "print(\"4. TOP-K SAMPLING (k=10) - Limits choices:\")\n",
    "print(\"-\" * 60)\n",
    "output4 = generate(model, tokenizer, prompt, max_length=150,\n",
    "                   temperature=1.0, top_k=10, device=device)\n",
    "print(output4)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ Text generation working! Try different prompts and settings.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ffb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function - Part 1\n",
    "def generate(model, tokenizer, prompt, max_length=100, temperature=1.0, \n",
    "             top_k=None, top_p=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LLaMA model\n",
    "        tokenizer: Character tokenizer\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum characters to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Keep only top k tokens (optional)\n",
    "        top_p: Nucleus sampling threshold (optional)\n",
    "        device: CPU or GPU\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    generated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get predictions\n",
    "            logits = model(tokens)\n",
    "            \n",
    "            # Get logits for last position\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "            generated.append(next_token.item())\n",
    "            \n",
    "            # Keep only last max_seq_len tokens\n",
    "            if tokens.size(1) > model.config.max_seq_len:\n",
    "                tokens = tokens[:, -model.config.max_seq_len:]\n",
    "    \n",
    "    # Decode and return\n",
    "    return prompt + tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35bf78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Generate Text\n",
    "\n",
    "## Step 13: Text Generation\n",
    "\n",
    "Now the fun part - making the model generate text!\n",
    "\n",
    "**How Generation Works:**\n",
    "1. Give model a starting text (prompt)\n",
    "2. Model predicts probability of each next character\n",
    "3. Sample a character based on probabilities\n",
    "4. Add character to sequence\n",
    "5. Repeat!\n",
    "\n",
    "**Sampling Strategies:**\n",
    "- **Temperature:** Controls randomness (low = safe, high = creative)\n",
    "- **Top-k:** Only consider k most likely options\n",
    "- **Top-p (Nucleus):** Consider top options until cumulative prob reaches p\n",
    "\n",
    "**Higher temperature = more creative (but less coherent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff840a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Initialize training\n",
    "model, optimizer, scheduler, criterion, start_time, global_step = train(\n",
    "    model, train_loader, num_epochs=5, device=device, learning_rate=3e-4\n",
    ")\n",
    "\n",
    "# Run training loop\n",
    "model = train_loop(\n",
    "    model, train_loader, num_epochs=5,\n",
    "    optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "    device=device, start_time=start_time, global_step=global_step\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training complete! Model is ready to generate text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample training text (you can replace with your own!)\n",
    "training_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "Machine learning is the study of computer algorithms that improve automatically through experience.\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\n",
    "A transformer is a deep learning model that adopts the mechanism of self-attention.\n",
    "LLaMA is a large language model developed by Meta AI.\n",
    "\"\"\" * 20  # Repeat to have more training data\n",
    "\n",
    "print(f\"Training text length: {len(training_text):,} characters\\n\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CharTokenizer(training_text)\n",
    "print(f\"Vocabulary: {tokenizer.vocab_size} unique characters\")\n",
    "print(f\"Characters: {''.join(tokenizer.chars[:30])}...\")\n",
    "\n",
    "# Update config with actual vocab size\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Recreate model with correct vocab size\n",
    "model = LLaMA(config).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel parameters: {n_params:,}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(training_text, tokenizer, seq_len=config.max_seq_len)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"\\nDataset: {len(dataset):,} training samples\")\n",
    "print(f\"Batches: {len(train_loader)} per epoch\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Sequence length: {config.max_seq_len}\")\n",
    "print(\"\\nâœ“ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81ee60",
   "metadata": {},
   "source": [
    "## Step 12: Load Real Training Data and Train\n",
    "\n",
    "Now let's prepare a real dataset and train the model!\n",
    "\n",
    "For this example, we'll use a sample text. In practice, you'd use much larger datasets like books or Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function - Part 2: Main loop\n",
    "def train_loop(model, train_loader, num_epochs, optimizer, scheduler, criterion, device, start_time, global_step):\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Calculate loss (reshape for CrossEntropyLoss)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (prevents exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                      f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {avg_loss:.4f} | LR: {lr:.6f}\")\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1} Complete | Avg Loss: {avg_epoch_loss:.4f} | Time: {elapsed:.1f}s\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING COMPLETE!\")\n",
    "    print(f\"Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Final loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce065f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function - Part 1: Setup\n",
    "def train(model, train_loader, num_epochs, device, learning_rate=3e-4):\n",
    "    \"\"\"\n",
    "    Train the LLaMA model.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaMA model to train\n",
    "        train_loader: DataLoader with training data\n",
    "        num_epochs: Number of times to go through dataset\n",
    "        device: CPU or GPU\n",
    "        learning_rate: Starting learning rate\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "    \n",
    "    # Setup learning rate scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = min(100, total_steps // 10)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"TRAINING STARTED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    return model, optimizer, scheduler, criterion, start_time, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877eb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Creates a learning rate scheduler with warmup and cosine decay.\n",
    "    \n",
    "    Warmup: Gradually increase LR (prevents instability at start)\n",
    "    Cosine: Smooth decrease (helps fine-tune at end)\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        # Warmup phase\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps)\n",
    "        )\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Visualize learning rate schedule\n",
    "print(\"EXAMPLE: Learning Rate Schedule\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dummy optimizer\n",
    "dummy_model = nn.Linear(10, 10)\n",
    "dummy_optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=1e-3)\n",
    "scheduler = get_cosine_schedule_with_warmup(dummy_optimizer, 100, 1000)\n",
    "\n",
    "# Sample learning rates\n",
    "lrs = []\n",
    "for step in range(1000):\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"Learning rate schedule (1000 steps, 100 warmup):\")\n",
    "print(f\"  Step 0:    {lrs[0]:.6f} (starting)\")\n",
    "print(f\"  Step 100:  {lrs[100]:.6f} (after warmup)\")\n",
    "print(f\"  Step 500:  {lrs[500]:.6f} (middle)\")\n",
    "print(f\"  Step 999:  {lrs[999]:.6f} (end)\")\n",
    "print(\"\\nâœ“ Learning rate starts low, peaks, then gradually decreases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf1c03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Training the Model\n",
    "\n",
    "## Step 11: Training Setup\n",
    "\n",
    "Now we train the model to predict next characters!\n",
    "\n",
    "**Training Process:**\n",
    "1. Show model some text\n",
    "2. Model tries to predict next character\n",
    "3. Compare prediction to actual next character\n",
    "4. Adjust model weights to improve\n",
    "5. Repeat thousands of times!\n",
    "\n",
    "**Key Components:**\n",
    "- **Optimizer:** AdamW (updates model weights intelligently)\n",
    "- **Learning Rate Scheduler:** Starts slow, then faster, then slower (cosine schedule)\n",
    "- **Loss Function:** CrossEntropyLoss (measures prediction accuracy)\n",
    "\n",
    "**Analogy:** Like learning to play piano - practice makes perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e75362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, text, tokenizer, seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Encode entire text\n",
    "        self.tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a training pair.\n",
    "        \n",
    "        Returns:\n",
    "            x: input sequence\n",
    "            y: target sequence (shifted by 1)\n",
    "        \"\"\"\n",
    "        x = self.tokens[idx:idx + self.seq_len]\n",
    "        y = self.tokens[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nEXAMPLE: Text Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset = TextDataset(sample_text, tokenizer, seq_len=10)\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Sequence length: 10 characters\")\n",
    "\n",
    "# Show example\n",
    "x, y = dataset[0]\n",
    "print(f\"\\nExample training pair:\")\n",
    "print(f\"  Input:  {x.tolist()}\")\n",
    "print(f\"  Target: {y.tolist()}\")\n",
    "print(f\"\\n  Input text:  '{tokenizer.decode(x.tolist())}'\")\n",
    "print(f\"  Target text: '{tokenizer.decode(y.tolist())}'\")\n",
    "print(\"\\n  Notice: Target is shifted by 1 position!\")\n",
    "print(\"  Model learns to predict the next character.\")\n",
    "print(\"\\nâœ“ Dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6241862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level Tokenizer\n",
    "class CharTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        # Find all unique characters\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token indices.\"\"\"\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert token indices back to text.\"\"\"\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])\n",
    "\n",
    "# Example: Create tokenizer with sample text\n",
    "sample_text = \"Hello World! This is a LLaMA model.\"\n",
    "tokenizer = CharTokenizer(sample_text)\n",
    "\n",
    "print(\"EXAMPLE: Character Tokenizer\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size} unique characters\")\n",
    "print(f\"Characters: {tokenizer.chars[:20]}...\")\n",
    "\n",
    "# Encode example\n",
    "test_text = \"Hello\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nEncoding test:\")\n",
    "print(f\"  Original: '{test_text}'\")\n",
    "print(f\"  Encoded:  {encoded}\")\n",
    "print(f\"  Decoded:  '{decoded}'\")\n",
    "print(f\"  Match: {test_text == decoded} âœ“\")\n",
    "print(\"\\nâœ“ Tokenizer working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a8422",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Prepare Training Data\n",
    "\n",
    "## Step 10: Tokenizer and Dataset\n",
    "\n",
    "Before training, we need to prepare our text data!\n",
    "\n",
    "**What's a Tokenizer?** Converts text â†” numbers\n",
    "- \"Hello\" â†’ [34, 56, 67, 67, 78]\n",
    "- Model only understands numbers!\n",
    "\n",
    "**For this demo:** We'll use character-level tokenization (each character = one token)\n",
    "- Simple and fast for learning\n",
    "- Real LLaMA uses more sophisticated tokenization (BPE)\n",
    "\n",
    "**Dataset:** Creates training pairs\n",
    "- Input: \"Hello worl\"\n",
    "- Target: \"ello world\" (shifted by 1)\n",
    "- Model learns to predict next character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc372b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the complete model\n",
    "print(\"CREATING COMPLETE LLaMA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = LLaMA(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_tokens = torch.randint(0, config.vocab_size, (2, 10)).to(device)\n",
    "print(f\"\\nInput tokens: {test_tokens.shape}\")\n",
    "print(f\"  (2 sentences, 10 tokens)\")\n",
    "\n",
    "mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_tokens, mask)\n",
    "\n",
    "print(f\"\\nOutput logits: {logits.shape}\")\n",
    "print(f\"  (2 sentences, 10 positions, {config.vocab_size} possible next tokens)\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  â€¢ Embedding layer: {config.vocab_size:,} tokens\")\n",
    "print(f\"  â€¢ {config.n_layers} transformer blocks\")\n",
    "print(f\"  â€¢ {config.n_heads} attention heads per block\")\n",
    "print(f\"  â€¢ {config.d_model} model dimension\")\n",
    "print(f\"  â€¢ {config.d_ff:,} FFN dimension\")\n",
    "print(\"\\nâœ“ Complete LLaMA model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete LLaMA Model\n",
    "class LLaMA(nn.Module):\n",
    "    \"\"\"Complete LLaMA language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.tok_embeddings = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            LLaMABlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = RMSNorm(config.d_model, config.rms_norm_eps)\n",
    "        \n",
    "        # Output projection (predict next token)\n",
    "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, tokens, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Inputs:\n",
    "            tokens: (batch, seq_len) - Token indices\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size) - Predictions\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        x = self.tok_embeddings(tokens)\n",
    "        \n",
    "        # Process through all layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final normalization and projection\n",
    "        x = self.norm(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613ddfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Complete LLaMA Model\n",
    "\n",
    "## Step 9: Build the Full Model\n",
    "\n",
    "Time to assemble everything! The complete LLaMA model has:\n",
    "\n",
    "**Architecture:**\n",
    "1. **Token Embedding** â†’ Convert words to numbers\n",
    "2. **N Transformer Blocks** â†’ Process and understand (we use 6)\n",
    "3. **Final Norm** â†’ Stabilize output\n",
    "4. **Output Head** â†’ Predict next word\n",
    "\n",
    "**Information Flow:**\n",
    "```\n",
    "Text â†’ Embedding â†’ Blockâ‚ â†’ Blockâ‚‚ â†’ ... â†’ Blockâ‚† â†’ Norm â†’ Prediction\n",
    "```\n",
    "\n",
    "Think of it like an assembly line - each block refines the understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Block\n",
    "class LLaMABlock(nn.Module):\n",
    "    \"\"\"Single transformer block with attention and feed-forward.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Attention components\n",
    "        self.attention = GroupedQueryAttention(config)\n",
    "        self.attention_norm = RMSNorm(config.d_model, config.rms_norm_eps)\n",
    "        \n",
    "        # Feed-forward components\n",
    "        self.feed_forward = SwiGLU(config)\n",
    "        self.ffn_norm = RMSNorm(config.d_model, config.rms_norm_eps)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Process through attention and feed-forward.\n",
    "        \n",
    "        Uses pre-normalization and residual connections.\n",
    "        \"\"\"\n",
    "        # Attention block with residual\n",
    "        h = x + self.attention(self.attention_norm(x), mask)\n",
    "        \n",
    "        # Feed-forward block with residual\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test transformer block\n",
    "print(\"EXAMPLE: Transformer Block\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "block = LLaMABlock(config).to(device)\n",
    "test_input = torch.randn(2, 10, config.d_model).to(device)\n",
    "mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Input:  {test_input.shape}\")\n",
    "output = block(test_input, mask)\n",
    "print(f\"Output: {output.shape}\")\n",
    "\n",
    "print(f\"\\nComponents in block:\")\n",
    "print(f\"  âœ“ Attention with {config.n_heads} heads\")\n",
    "print(f\"  âœ“ SwiGLU feed-forward\")  \n",
    "print(f\"  âœ“ RMSNorm (2x)\")\n",
    "print(f\"  âœ“ Residual connections (2x)\")\n",
    "print(\"\\nâœ“ Complete transformer block working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745449d4",
   "metadata": {},
   "source": [
    "## Step 8: Complete Transformer Block\n",
    "\n",
    "Now we combine everything into a single **transformer block**!\n",
    "\n",
    "**A Block Contains:**\n",
    "1. **Attention Layer** â†’ Looks at relationships between words\n",
    "2. **Feed-Forward Layer** â†’ Processes each word independently  \n",
    "3. **Normalization** â†’ Keeps numbers stable (before each layer)\n",
    "4. **Residual Connections** â†’ Helps information flow (adds input to output)\n",
    "\n",
    "**Residual Connections:** Like a highway bypass - if the layer doesn't help, just skip it!\n",
    "\n",
    "**Formula:** \n",
    "- x = x + Attention(Norm(x))\n",
    "- x = x + FFN(Norm(x))\n",
    "\n",
    "LLaMA stacks multiple blocks (we'll use 6) to build the complete model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwiGLU Feed-Forward Network\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"Gated feed-forward network with SwiGLU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        hidden_dim = config.d_ff\n",
    "        \n",
    "        # Three transformations\n",
    "        self.w1 = nn.Linear(config.d_model, hidden_dim, bias=False)  # Gate\n",
    "        self.w2 = nn.Linear(hidden_dim, config.d_model, bias=False)  # Output\n",
    "        self.w3 = nn.Linear(config.d_model, hidden_dim, bias=False)  # Value\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply SwiGLU transformation.\n",
    "        \n",
    "        Formula: w2(silu(w1(x)) * w3(x))\n",
    "        where silu(x) = x * sigmoid(x)\n",
    "        \"\"\"\n",
    "        # Gate path (with SiLU activation)\n",
    "        gate = F.silu(self.w1(x))\n",
    "        \n",
    "        # Value path (no activation)\n",
    "        value = self.w3(x)\n",
    "        \n",
    "        # Combine with gating\n",
    "        hidden = gate * value\n",
    "        \n",
    "        # Project back to original dimension\n",
    "        return self.dropout(self.w2(hidden))\n",
    "\n",
    "# Example usage\n",
    "print(\"EXAMPLE: SwiGLU Activation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "swiglu = SwiGLU(config).to(device)\n",
    "test_input = torch.randn(2, 10, config.d_model).to(device)\n",
    "\n",
    "print(f\"Input:  {test_input.shape}\")\n",
    "print(f\"        (2 sentences, 10 words, {config.d_model} dims)\")\n",
    "\n",
    "output = swiglu(test_input)\n",
    "\n",
    "print(f\"\\nOutput: {output.shape}\")\n",
    "print(f\"        (same shape, but transformed)\")\n",
    "print(f\"\\nSample input values:  {test_input[0, 0, :5]}\")\n",
    "print(f\"Sample output values: {output[0, 0, :5]}\")\n",
    "print(\"\\nâœ“ SwiGLU adds complex non-linear transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e399af9",
   "metadata": {},
   "source": [
    "## Step 7: SwiGLU Activation Function\n",
    "\n",
    "**What's an Activation Function?** It adds \"non-linearity\" so models can learn complex patterns.\n",
    "\n",
    "**Why SwiGLU?** Better than older activations like ReLU or GELU.\n",
    "\n",
    "**The Magic:** Uses a \"gate\" mechanism\n",
    "- One path processes the information\n",
    "- Another path decides what to let through\n",
    "- Like a bouncer at a club - decides what gets in!\n",
    "\n",
    "**Technical:** SwiGLU = Swish(xW) âŠ— (xV)\n",
    "- âŠ— means element-wise multiplication\n",
    "- W and V are learnable transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Grouped Query Attention\n",
    "print(\"EXAMPLE: Testing Grouped Query Attention\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gqa = GroupedQueryAttention(config).to(device)\n",
    "test_x = torch.randn(2, 10, config.d_model).to(device)\n",
    "\n",
    "print(f\"Input: {test_x.shape}\")\n",
    "print(f\"       (2 sentences, 10 words, {config.d_model} dimensions)\")\n",
    "\n",
    "# Create causal mask (prevent looking at future words)\n",
    "causal_mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "print(f\"\\nCausal mask: {causal_mask.shape}\")\n",
    "print(f\"First 5x5 of mask:\\n{causal_mask[0, 0, :5, :5].int()}\")\n",
    "print(\"(1 = can attend, 0 = cannot attend)\")\n",
    "\n",
    "output = gqa(test_x, mask=causal_mask)\n",
    "\n",
    "print(f\"\\nOutput: {output.shape}\")\n",
    "print(f\"\\nMemory Savings:\")\n",
    "print(f\"  Query heads:     {gqa.n_heads}\")\n",
    "print(f\"  Key/Value heads: {gqa.n_kv_heads}\")\n",
    "print(f\"  Savings:         {(1 - gqa.n_kv_heads/gqa.n_heads)*100:.0f}%\")\n",
    "print(\"\\nâœ“ Grouped Query Attention working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Query Attention - Part 1: Setup\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Memory-efficient attention with shared K/V heads.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads        # Query heads (8)\n",
    "        self.n_kv_heads = config.n_kv_heads  # K/V heads (4)\n",
    "        self.head_dim = config.head_dim\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # How many Q heads per K/V head?\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        \n",
    "        # Projection layers (note: K/V have fewer heads!)\n",
    "        self.wq = nn.Linear(self.d_model, self.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads * self.head_dim, self.d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Store RoPE frequencies\n",
    "        self.register_buffer(\n",
    "            \"rope_freqs\",\n",
    "            precompute_rope_freqs(self.head_dim, config.max_seq_len, config.rope_theta)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Apply grouped query attention.\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q = apply_rope(q, self.rope_freqs)\n",
    "        k = apply_rope(k, self.rope_freqs)\n",
    "        \n",
    "        # Repeat K/V to match Q heads\n",
    "        if self.n_rep > 1:\n",
    "            k = k.repeat_interleave(self.n_rep, dim=1)\n",
    "            v = v.repeat_interleave(self.n_rep, dim=1)\n",
    "        \n",
    "        # Calculate attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply to values and combine heads\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed2679",
   "metadata": {},
   "source": [
    "## Step 6: Grouped Query Attention (GQA)\n",
    "\n",
    "**The Big Idea:** Attention helps models focus on important words, but uses lots of memory!\n",
    "\n",
    "**The Problem:** Traditional attention creates 3 copies (Q, K, V) for EACH head.\n",
    "- 8 heads = 24 separate copies = lots of memory!\n",
    "\n",
    "**The Smart Solution (GQA):** Share Key and Value copies across multiple Query heads.\n",
    "- 8 Query heads, but only 4 Key/Value heads\n",
    "- Each K/V pair serves 2 Q heads\n",
    "- **Result:** 50% memory savings!\n",
    "\n",
    "**Real-world analogy:**\n",
    "- Old way: 8 students, each with their own textbook ($$$)\n",
    "- New way: 8 students sharing 4 textbooks (saves money, same learning!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f118ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm implementation\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Prevent division by zero\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # Learnable scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Normalize input.\n",
    "        \n",
    "        Input:  x with shape (..., dim)\n",
    "        Output: Normalized x (same shape)\n",
    "        \"\"\"\n",
    "        # Calculate RMS: sqrt(mean(xÂ²))\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        # Normalize and scale\n",
    "        return self.weight * (x / rms)\n",
    "\n",
    "# Example usage\n",
    "print(\"EXAMPLE: Using RMSNorm\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rms_norm = RMSNorm(dim=8)\n",
    "test_input = torch.randn(2, 5, 8)  # 2 sentences, 5 words, 8 dims\n",
    "\n",
    "print(\"BEFORE RMSNorm:\")\n",
    "print(f\"  Shape: {test_input.shape}\")\n",
    "print(f\"  First word values: {test_input[0, 0, :4]}\")\n",
    "print(f\"  Mean: {test_input[0, 0].mean():.4f}\")\n",
    "print(f\"  Std:  {test_input[0, 0].std():.4f}\")\n",
    "\n",
    "output = rms_norm(test_input)\n",
    "\n",
    "print(\"\\nAFTER RMSNorm:\")\n",
    "print(f\"  Shape: {output.shape}\")\n",
    "print(f\"  First word values: {output[0, 0, :4]}\")\n",
    "print(f\"  RMS (should be ~1.0): {torch.sqrt((output[0, 0]**2).mean()):.4f}\")\n",
    "print(\"\\nâœ“ Numbers are now balanced and stable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa42a4f",
   "metadata": {},
   "source": [
    "## Step 5: RMSNorm - Root Mean Square Normalization\n",
    "\n",
    "**The Problem:** During training, numbers can become too big or too small, causing issues.\n",
    "\n",
    "**The Solution:** RMSNorm rescales all numbers to a reasonable range.\n",
    "\n",
    "**Simple analogy:** Like adjusting audio volume - too loud is distorted, too quiet can't be heard. RMSNorm keeps the \"volume\" just right!\n",
    "\n",
    "**How it works:**\n",
    "1. Calculate the average size (RMS) of all numbers\n",
    "2. Divide all numbers by this average\n",
    "3. Multiply by a learnable scale factor\n",
    "\n",
    "**Why RMSNorm?** Faster than LayerNorm (skips mean-centering step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RoPE rotations to data\n",
    "def apply_rope(x, freqs):\n",
    "    \"\"\"\n",
    "    Apply rotations to word representations.\n",
    "    \n",
    "    Inputs:\n",
    "        x: Tensor (batch, n_heads, seq_len, head_dim)\n",
    "        freqs: Precomputed rotation values\n",
    "    \n",
    "    Returns:\n",
    "        Rotated tensor (same shape as input)\n",
    "    \"\"\"\n",
    "    batch, n_heads, seq_len, head_dim = x.shape\n",
    "    \n",
    "    # Separate even/odd dimensions\n",
    "    x_reshaped = x.reshape(batch, n_heads, seq_len, head_dim // 2, 2)\n",
    "    x_even = x_reshaped[..., 0]  # Elements at indices 0, 2, 4, ...\n",
    "    x_odd = x_reshaped[..., 1]   # Elements at indices 1, 3, 5, ...\n",
    "    \n",
    "    # Get cos/sin values\n",
    "    cos = freqs[:seq_len, :, 0].unsqueeze(0).unsqueeze(0)\n",
    "    sin = freqs[:seq_len, :, 1].unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply 2D rotation\n",
    "    rotated_even = x_even * cos - x_odd * sin\n",
    "    rotated_odd = x_even * sin + x_odd * cos\n",
    "    \n",
    "    # Combine back\n",
    "    rotated = torch.stack([rotated_even, rotated_odd], dim=-1)\n",
    "    return rotated.reshape(batch, n_heads, seq_len, head_dim)\n",
    "\n",
    "# Test RoPE\n",
    "print(\"\\nEXAMPLE: Applying RoPE\")\n",
    "print(\"=\" * 60)\n",
    "test_data = torch.randn(2, 4, 10, 32)\n",
    "print(f\"Input:  {test_data.shape}\")\n",
    "print(f\"        (2 sentences, 4 heads, 10 words, 32 dimensions)\")\n",
    "\n",
    "rotated = apply_rope(test_data, rope_freqs)\n",
    "print(f\"\\nOutput: {rotated.shape}\")\n",
    "print(f\"        (same shape, but values rotated based on position)\")\n",
    "print(f\"\\nChanged: {not torch.equal(test_data, rotated)}\")\n",
    "print(\"\\nâœ“ RoPE applied! Model now knows word positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RoPE rotation frequencies\n",
    "def precompute_rope_freqs(head_dim, max_seq_len, theta=10000.0):\n",
    "    \"\"\"\n",
    "    Precompute rotation frequencies for each position.\n",
    "    \n",
    "    Inputs:\n",
    "        head_dim: Size of each attention head (e.g., 32)\n",
    "        max_seq_len: Max sequence length (e.g., 128)\n",
    "        theta: Rotation speed control (default: 10000)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with cos/sin values for each position\n",
    "    \"\"\"\n",
    "    # Calculate frequencies\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    \n",
    "    # Position indices: 0, 1, 2, ..., max_seq_len-1\n",
    "    positions = torch.arange(max_seq_len).float()\n",
    "    \n",
    "    # Outer product: position Ã— frequency\n",
    "    freqs = torch.outer(positions, freqs)\n",
    "    \n",
    "    # Convert to cos and sin (for rotation)\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    \n",
    "    return torch.stack([freqs_cos, freqs_sin], dim=-1)\n",
    "\n",
    "# Example usage\n",
    "print(\"EXAMPLE: Creating RoPE Frequencies\")\n",
    "print(\"=\" * 60)\n",
    "rope_freqs = precompute_rope_freqs(head_dim=32, max_seq_len=10)\n",
    "print(f\"Input:  head_dim=32, max_seq_len=10\")\n",
    "print(f\"Output: shape {rope_freqs.shape}\")\n",
    "print(f\"        (10 positions, 16 freq pairs, 2 values [cos,sin])\")\n",
    "print(f\"\\nPosition 0 (first 3 frequency pairs):\")\n",
    "print(rope_freqs[0, :3])\n",
    "print(f\"\\nPosition 5 (first 3 frequency pairs):\")\n",
    "print(rope_freqs[5, :3])\n",
    "print(\"\\nâœ“ Each position has unique rotation values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f5133",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Build Model Components\n",
    "\n",
    "Now we'll build each piece of the LLaMA model, one at a time, with examples!\n",
    "\n",
    "## Step 4: RoPE - Rotary Position Embeddings\n",
    "\n",
    "**The Problem:** Computers don't naturally understand word order.\n",
    "- \"Dog bites man\" â‰  \"Man bites dog\"\n",
    "\n",
    "**The Solution:** RoPE gives each position a unique \"rotation\" so the model knows word order.\n",
    "\n",
    "**How it works:**\n",
    "1. Each position (0, 1, 2, ...) gets a rotation angle\n",
    "2. We apply these rotations to the word representations\n",
    "3. The model can now tell which words come before/after others\n",
    "\n",
    "**Why RoPE is better:** Works great for long texts and is more efficient than older methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f49777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    \"\"\"All settings for our LLaMA model.\"\"\"\n",
    "    \n",
    "    # Model architecture\n",
    "    vocab_size: int = 512       # Number of unique characters (updated later)\n",
    "    d_model: int = 256          # Size of word representations\n",
    "    n_layers: int = 6           # Number of transformer blocks\n",
    "    n_heads: int = 8            # Attention heads for Queries\n",
    "    n_kv_heads: int = 4         # Attention heads for Keys/Values (saves memory!)\n",
    "    d_ff: int = 1024            # Feed-forward network size\n",
    "    \n",
    "    # Training settings\n",
    "    max_seq_len: int = 128      # Maximum text length\n",
    "    dropout: float = 0.1        # Regularization (prevents overfitting)\n",
    "    \n",
    "    # Technical parameters\n",
    "    rope_theta: float = 10000.0       # RoPE rotation parameter\n",
    "    rms_norm_eps: float = 1e-6        # Numerical stability constant\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate settings.\"\"\"\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        assert self.n_heads % self.n_kv_heads == 0\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "\n",
    "# Create configuration\n",
    "config = LLaMAConfig()\n",
    "\n",
    "# Display settings\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Vocabulary size:      {config.vocab_size:>6,} characters\")\n",
    "print(f\"Model dimension:      {config.d_model:>6}\")\n",
    "print(f\"Number of layers:     {config.n_layers:>6}\")\n",
    "print(f\"Attention heads (Q):  {config.n_heads:>6}\")\n",
    "print(f\"Attention heads (KV): {config.n_kv_heads:>6}\")\n",
    "print(f\"Head dimension:       {config.head_dim:>6}\")\n",
    "print(f\"FFN dimension:        {config.d_ff:>6,}\")\n",
    "print(f\"Max sequence length:  {config.max_seq_len:>6}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ’¡ These are small values for fast training!\")\n",
    "print(\"   Real LLaMA uses much bigger numbers.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e15f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Model Configuration\n",
    "\n",
    "## Step 3: Define Model Settings\n",
    "\n",
    "Before building, we need to decide the model's \"size\" and settings.\n",
    "\n",
    "**Think of it like building a house:**\n",
    "- How many floors? (layers)\n",
    "- How big are the rooms? (dimensions)\n",
    "- How many windows? (attention heads)\n",
    "\n",
    "We'll use small numbers so training is fast on your computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ebfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Detect available hardware\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"ðŸŽ Using: Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"ðŸŽ® Using: NVIDIA GPU - {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"ðŸ’» Using: CPU (slower, but will work)\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "# Test the device with a simple tensor\n",
    "test = torch.ones(3, 3).to(device)\n",
    "print(f\"\\nTest tensor created on {device}:\")\n",
    "print(test)\n",
    "print(\"\\nâœ“ Device is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0ccf9",
   "metadata": {},
   "source": [
    "## Step 2: Setup Computing Device\n",
    "\n",
    "AI models run faster on different hardware:\n",
    "- **CPU** â†’ Your computer's main processor (slower)\n",
    "- **NVIDIA GPU (CUDA)** â†’ Graphics card (much faster!)\n",
    "- **Apple Silicon (MPS)** â†’ M1/M2/M3 chips (also fast!)\n",
    "\n",
    "Let's detect what you have and use the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Verify imports worked\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ ALL IMPORTS SUCCESSFUL!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version:  {sys.version.split()[0]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c44cbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Setup and Imports\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, we need to load all the Python tools (libraries) we'll use.\n",
    "\n",
    "**What each library does:**\n",
    "- `torch` â†’ Main deep learning framework\n",
    "- `torch.nn` â†’ Neural network building blocks\n",
    "- `torch.nn.functional` â†’ Mathematical operations\n",
    "- `math` â†’ Basic math functions\n",
    "- `time` â†’ Track training duration\n",
    "- `Dataset, DataLoader` â†’ Handle training data\n",
    "- `dataclass` â†’ Easy configuration setup\n",
    "- `typing` â†’ Type hints for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c3c93",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Build and Train a LLaMA Model from Scratch\n",
    "\n",
    "You've successfully built and trained a LLaMA model from scratch!\n",
    "\n",
    "## What You Accomplished:\n",
    "\n",
    "âœ… **Built a complete transformer model** with modern architecture\n",
    "- RoPE for position encoding\n",
    "- RMSNorm for stabilization  \n",
    "- Grouped Query Attention for efficiency\n",
    "- SwiGLU activation function\n",
    "\n",
    "âœ… **Trained the model** on text data\n",
    "- Character-level tokenization\n",
    "- AdamW optimizer with cosine scheduling\n",
    "- Proper gradient clipping\n",
    "\n",
    "âœ… **Generated text** with different sampling strategies\n",
    "- Temperature control\n",
    "- Top-k and top-p sampling\n",
    "\n",
    "âœ… **Evaluated and saved** your model\n",
    "- Perplexity metrics\n",
    "- Checkpoint system\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### To Improve Your Model:\n",
    "1. **More Training Data:** Use larger datasets (books, Wikipedia, etc.)\n",
    "2. **Longer Training:** Train for more epochs\n",
    "3. **Bigger Model:** Increase `d_model`, `n_layers`, etc.\n",
    "4. **Better Tokenization:** Use BPE or SentencePiece instead of characters\n",
    "5. **Fine-tuning:** Train on specific tasks or domains\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "- **Multi-GPU Training:** Distribute training across GPUs\n",
    "- **Mixed Precision:** Use FP16 for faster training\n",
    "- **LoRA:** Efficient fine-tuning technique\n",
    "- **RLHF:** Reinforcement Learning from Human Feedback\n",
    "- **Prompt Engineering:** Optimize prompts for better outputs\n",
    "\n",
    "### Real LLaMA Models:\n",
    "This demo used small sizes for learning. Real LLaMA models:\n",
    "- LLaMA 7B: 7 billion parameters\n",
    "- LLaMA 13B: 13 billion parameters  \n",
    "- LLaMA 70B: 70 billion parameters\n",
    "\n",
    "Your model: ~{n_params:,} parameters (much smaller for fast training!)\n",
    "\n",
    "## Resources:\n",
    "- [LLaMA Paper](https://arxiv.org/abs/2302.13971)\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You now understand how modern large language models work! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
